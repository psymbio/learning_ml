{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be9cc10e-0394-45b3-91bc-7b0ce9f61761",
   "metadata": {},
   "source": [
    "Some important pointers\n",
    "\n",
    "1. Vocabulary Initialization and Indexing:\n",
    "You are initializing the vocabulary with a padding token, but you are not handling out-of-vocabulary (OOV) tokens properly. If a word in the test sentence is not in the vocabulary, it will raise a KeyError.\n",
    "\n",
    "2. Handling Zero Probabilities:\n",
    "In Naive Bayes, multiplying probabilities can lead to underflow issues. To avoid this, it's common to use log probabilities instead of raw probabilities.\n",
    "\n",
    "3. Prior Probabilities:\n",
    "You are assuming equal prior probabilities for both classes (0.5 for each). However, it's better to calculate the prior probabilities based on the actual distribution of the classes in the training data.\n",
    "\n",
    "4. Smoothing:\n",
    "You are adding a very small constant (0.00000000000001) to avoid division by zero. However, it's better to use Laplace smoothing (add-one smoothing) to handle zero probabilities more robustly.\n",
    "\n",
    "5. Efficiency:\n",
    "The code can be optimized by avoiding redundant computations and using vectorized operations where possible.\n",
    "\n",
    "6. Error Handling:\n",
    "The error handling in transform_sentence and transform_test_sentence is not robust. If a sentence is empty or contains no valid tokens, it should return an empty list or handle it gracefully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69aeff46-d6fc-4c23-a27b-d08809725ec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/vanilla_skies/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/vanilla_skies/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/vanilla_skies/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import defaultdict\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6207e00-9a90-4cea-97b2-3478080d8c9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>email</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>date wed NUMBER aug NUMBER NUMBER NUMBER NUMB...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>martin a posted tassos papadopoulos the greek ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>man threatens explosion in moscow thursday aug...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>klez the virus that won t die already the most...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>in adding cream to spaghetti carbonara which ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2995</th>\n",
       "      <td>abc s good morning america ranks it the NUMBE...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2996</th>\n",
       "      <td>hyperlink hyperlink hyperlink let mortgage le...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2997</th>\n",
       "      <td>thank you for shopping with us gifts for all ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2998</th>\n",
       "      <td>the famous ebay marketing e course learn to s...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2999</th>\n",
       "      <td>hello this is chinese traditional 子 件 NUMBER世...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2999 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  email  label\n",
       "0      date wed NUMBER aug NUMBER NUMBER NUMBER NUMB...      0\n",
       "1     martin a posted tassos papadopoulos the greek ...      0\n",
       "2     man threatens explosion in moscow thursday aug...      0\n",
       "3     klez the virus that won t die already the most...      0\n",
       "4      in adding cream to spaghetti carbonara which ...      0\n",
       "...                                                 ...    ...\n",
       "2995   abc s good morning america ranks it the NUMBE...      1\n",
       "2996   hyperlink hyperlink hyperlink let mortgage le...      1\n",
       "2997   thank you for shopping with us gifts for all ...      1\n",
       "2998   the famous ebay marketing e course learn to s...      1\n",
       "2999   hello this is chinese traditional 子 件 NUMBER世...      1\n",
       "\n",
       "[2999 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"spam_or_not_spam.csv\")\n",
    "df.dropna(inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24853ab2-59e9-46c8-b114-582b0baa8093",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize stop words and lemmatizer\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "pattern = r\"[^a-zA-Z-]\"\n",
    "\n",
    "# Initialize vocabulary and index\n",
    "vocab, index = {}, 1\n",
    "vocab['<pad>'] = 0  # Padding token\n",
    "vocab['<unk>'] = index  # Unknown token\n",
    "index += 1\n",
    "\n",
    "# Function to transform sentence\n",
    "def transform_sentence(sentence):\n",
    "    global vocab, index\n",
    "    try:\n",
    "        transformed_sentence = sentence.lower()\n",
    "        transformed_sentence = transformed_sentence.translate(str.maketrans('', '', string.punctuation))\n",
    "        transformed_sentence = word_tokenize(transformed_sentence)\n",
    "        transformed_sentence = [\n",
    "            lemmatizer.lemmatize(w)\n",
    "            for w in transformed_sentence\n",
    "            if w not in stop_words and not re.search(pattern, w)\n",
    "        ]\n",
    "        for token in transformed_sentence:\n",
    "            if token not in vocab:\n",
    "                vocab[token] = index\n",
    "                index += 1\n",
    "        transformed_sentence = [vocab.get(word, vocab['<unk>']) for word in transformed_sentence]\n",
    "    except Exception as e:\n",
    "        print(e, sentence)\n",
    "        return [vocab['<unk>']]\n",
    "    return transformed_sentence\n",
    "\n",
    "# Transform all sentences in the dataset\n",
    "features = [transform_sentence(row['email']) for _, row in df.iterrows()]\n",
    "labels = list(df[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed9d3fc0-1bbf-45eb-b71a-cc7cfe2abd09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate prior probabilities\n",
    "prior_pos = labels.count(0) / len(labels)\n",
    "prior_neg = labels.count(1) / len(labels)\n",
    "\n",
    "# Calculate word frequencies with Laplace smoothing\n",
    "vocab_pos_neg_freq = defaultdict(lambda: [1, 1])  # Laplace smoothing\n",
    "for i in range(len(features)):\n",
    "    for token in features[i]:\n",
    "        if labels[i] == 0:\n",
    "            vocab_pos_neg_freq[token][0] += 1\n",
    "        else:\n",
    "            vocab_pos_neg_freq[token][1] += 1\n",
    "\n",
    "# Normalize frequencies to get probabilities\n",
    "for key in vocab_pos_neg_freq:\n",
    "    total_pos = vocab_pos_neg_freq[key][0] + vocab_pos_neg_freq[key][1]\n",
    "    vocab_pos_neg_freq[key][0] /= total_pos\n",
    "    vocab_pos_neg_freq[key][1] /= total_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3cc47166-a69b-4d6f-8437-690f8df45caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to transform test sentence\n",
    "def transform_test_sentence(sentence):\n",
    "    global vocab\n",
    "    try:\n",
    "        transformed_sentence = sentence.lower()\n",
    "        transformed_sentence = transformed_sentence.translate(str.maketrans('', '', string.punctuation))\n",
    "        transformed_sentence = word_tokenize(transformed_sentence)\n",
    "        transformed_sentence = [\n",
    "            lemmatizer.lemmatize(w)\n",
    "            for w in transformed_sentence\n",
    "            if w not in stop_words and not re.search(pattern, w)\n",
    "        ]\n",
    "        transformed_sentence = [vocab.get(word, vocab['<unk>']) for word in transformed_sentence]\n",
    "    except Exception as e:\n",
    "        print(e, sentence)\n",
    "        return [vocab['<unk>']]\n",
    "    return transformed_sentence\n",
    "\n",
    "# Function to classify a test sentence and return the predicted class\n",
    "def classify_sentence(sentence):\n",
    "    transformed_sentence = transform_test_sentence(sentence)\n",
    "    log_pos, log_neg = np.log(prior_pos), np.log(prior_neg)\n",
    "    for token in transformed_sentence:\n",
    "        pos_freq, neg_freq = vocab_pos_neg_freq[token]\n",
    "        log_pos += np.log(pos_freq)\n",
    "        log_neg += np.log(neg_freq)\n",
    "    \n",
    "    # Determine the predicted class\n",
    "    if log_pos > log_neg:\n",
    "        return \"Not Spam\"\n",
    "    else:\n",
    "        return \"Spam\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e170371-dbc2-45b1-9a51-5046cd28faa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Spam\n",
      "Prediction: Spam\n"
     ]
    }
   ],
   "source": [
    "# Test examples\n",
    "test_example = \"give me money i need money this is a ad money chinese fake spam buy this buy this viagra viagra fuck me suck me hot girls online ad ad ad hyperlink hyperlink hyperlink\"\n",
    "prediction = classify_sentence(test_example)\n",
    "print(f\"Prediction: {prediction}\")\n",
    "\n",
    "test_example = df[\"email\"][2996]\n",
    "prediction = classify_sentence(test_example)\n",
    "print(f\"Prediction: {prediction}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db35dcd-be61-4257-8703-40ad96073675",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
